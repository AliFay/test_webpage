<span style="font-weight: 400;">This article is a follow-up of a previously published blog post titled as “Accelerated Development of Learned Agents for Urban Autonomous Driving” (please see [</span><a href="http://auro.ai/blog/2020/05/accelerated-development-of-learned-agents-for-urban-autonomous-driving/"><span style="font-weight: 400;">1</span></a><span style="font-weight: 400;">]). It deepens the explanation of our behavior-based actor controller that reproduces the human-like behaviors. This actor controller is mainly intended for reproducing learning scenes in driving simulators from which </span><a href="http://auro.ai/blog/2019/12/a-modular-vehicle-agnostic-sensor-housing-unit-for-autonomous-vehicle-platforms/"><span style="font-weight: 400;">our agents</span></a> <a href="http://auro.ai/blog/2020/05/learning-to-drive/"><span style="font-weight: 400;">learn </span></a><span style="font-weight: 400;">how to autonomously and safely navigate in the real world. The structure of this introductory article is as follows:</span>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The necessity of diverse and human-like behaviors in micro-simulations is briefly discussed.</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">A brief overview of our modular and layered architecture governing our actor controller’s behaviors is presented. </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Two examples of driving behaviors in presence of the signalized intersections are presented to express: </span></li>
</ul>
 	<ol style="list-style-type: none;">
<ol>
 	<li><span style="font-weight: 400;">How we adapt Behavior Trees (BT) - extensively used in video games - to realize diverse driving behaviors. </span></li>
 	<li><span style="font-weight: 400;">How we use real traffic data to drive our customized simulated actors. </span></li>
</ol>
</ol>
<h3><span style="font-weight: 400;">Why do we need human-like behaviors in a traffic micro-simulation? </span></h3>
<span style="font-weight: 400;">Considering the gradual proliferation of autonomous vehicles, the most likely scenario in the upcoming decades would be autonomous vehicles sharing the roadways with human-operated vehicles [</span><a href="https://journals.sagepub.com/doi/pdf/10.3141/2625-06?casa_token=OuLemAD14rMAAAAA:C-c3y6GHWegcBg-D4bJ0oJVR4d5mjoi7_eMDt0zPB8i-aIpKFJk0G6ll3jwaTZEuJIs_E33X-k8M"><span style="font-weight: 400;">2</span></a><span style="font-weight: 400;">]. The human driver behavior is heterogeneous in the sense that different drivers have different driving styles and attributes [</span><a href="https://www.researchgate.net/profile/Manos_Chaniotakis/publication/340424972_Factors_affecting_traffic_flow_efficiency_implications_of_connected_and_autonomous_vehicles_A_review_and_policy_recommendations/links/5e8b18aa92851c2f528329a4/Factors-affecting-traffic-flow-efficiency-implications-of-connected-and-autonomous-vehicles-A-review-and-policy-recommendations.pdf"><span style="font-weight: 400;">3</span></a><span style="font-weight: 400;">]. Under the same surrounding driving conditions, different drivers may make different decisions, mainly because of our various levels of aggression, attention, frustration, and experience. In addition, human drivers are fairly unpredictable by nature as we make not only mistakes and violations but also evasive maneuvers when we anticipate mistakes of other drivers. All of this introduces additional challenges in creating the appropriate level of behavioral complexity in a simulation environment that is intended for training and testing an autonomous vehicle. </span>
<h3><span style="font-weight: 400;">Generic Approaches and Limitations</span></h3>
<span style="font-weight: 400;">In order to provide the autonomous driving agent (ego vehicle) with an appropriate simulation environment, one may consider different approaches:</span>
<ul>
 	<li style="font-weight: 400;"><b>Re-simulation of other actors’ exact trajectories</b><span style="font-weight: 400;">: The real-world trajectories of the surrounding actors are recorded and replayed as-is in simulation. This may be appropriate for the validation of an existing autonomous driving agent, but does not provide the appropriate level of behavioral complexity needed for training an agent from scratch [</span><a href="http://auro.ai/blog/2020/05/accelerated-development-of-learned-agents-for-urban-autonomous-driving/"><span style="font-weight: 400;">1</span></a><span style="font-weight: 400;">].</span></li>
 	<li style="font-weight: 400;"><b>Passive scenario generation using randomized traffic</b><span style="font-weight: 400;">: One could also play out a mixed-traffic microsimulation consisting of multiple autonomous agents and human-controlled actor models, and let a scenario classification engine monitor the simulation whether a desired scenario has happened or not. In addition to being a tool for the validation of an existing autonomous driving agent, this approach may also be beneficial for finding/testing new emergent scenarios that were not planned to be monitored or collected.</span></li>
</ul>
<h2><span style="font-weight: 400;">Our Approach</span></h2>
<span style="font-weight: 400;"><b>Targeted scenario generation by specifying actors’ initial conditions and intended behaviors</b>: Only the initial conditions of a real-world scenario are replicated in the simulation environment, along with the intended behavior or destination for each agent. This allows natural interactions between the actors to emerge as the simulation is running. This is the approach we are presenting in this article; and, we use traffic modeling so that actors' decisions and resulting interactions are as close as possible to what the ego vehicle would experience in real-world scenarios, sharing the roadways with human-operated vehicles.</span>

<span style="font-weight: 400;">We represent a wide range of human driving behaviors by designing a custom actor model and integrating that into a scenario-based simulation execution engine. We designed the architecture of this scenario execution engine to be simulation-agnostic, that can be integrated easily into a simulation platform of choice like Applied Intuition or Carla. In this blog post, we present our integration on the opensource </span><a href="http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf"><span style="font-weight: 400;">CARLA</span></a><span style="font-weight: 400;"> simulator. The ego vehicle is controlled by a reference autonomous vehicle (AV) <a href="https://auro.ai/blog/2019/08/using-open-source-frameworks-in-autonomous-vehicle-development-part-1/">software stack</a> and is exposed to different driving situations such as car-following, lane-change, signalized and unsignalized intersection situations (see figure below). We include tunable parameters to add complexity, imperfection, and uncertainty to each behavior. Real-world distributions for these parameters are extracted from recorded naturalistic driving datasets, and the parameter values for each actor are sampled from those distributions. This provides a comprehensive toolchain for training, testing and validating the performance of the AV stack in dynamic driving scenarios.</span>
<p style="text-align: center;"><img class=" wp-image-771" src="https://auro.ai/wp-content/uploads/2020/10/figure_1_v1.png" alt="Different driving situations between the ego and the behavior-based custom actor" width="100%" /><em>Some examples of the different driving situations that our autonomous agent (the ego white vehicle) is exposed to in simulation. The ego is trained by being surrounded by the behavior-based actors with human-like behaviors (the blue vehicles).</em></p>

<div class="mceTemp"></div>
&nbsp;
<h2><span style="font-weight: 400;">Modular Architecture  </span></h2>
<span style="font-weight: 400;">A modular architecture allows us to model a set of behavior-based actors that each can carry out one or more behaviors independently. We adapt Behavior Trees (BT) - a computational model of plan execution extensively used in video games, and gaining popularity in autonomous systems - to realize sequential and parallel combinations of actor behaviors (see an example implementation in the next section).</span>

<span style="font-weight: 400;">The module hierarchy in our behavior-based actor model, as shown in the figure below, looks similar to a full AV stack, and some of the modules may indeed be interchangeable. The key difference is that the goal of an AV stack is to produce optimal and safe driving behavior, while the goal of our actor model is to reproduce realistic human behavior (which, as we all know, is not always optimal or safe!). A brief description of the key processing layers (Perception, Planning and Controls) is provided below.</span>

<img class="wp-image-816 alignright" src="https://auro.ai/wp-content/uploads/2020/10/Behavior_Modeling_Overview_v4.png" alt="" width="48%" />
<ul>
 	<li style="font-weight: 400;"><b>Perception</b><span style="font-weight: 400;">: Actors may have a fully operational sensing system that consumes data produced by different artificial sensors (cameras, LiDARs, RADARs, GPS, etc.). In this case, the perception module will need to process the sensory data and produce perceived objects, and also localize the actor in the environment. But that would incur a significant computational load, especially when we have multiple actors in a scene. Moreover, it can introduce undesired failures and make it harder to guide actor behavior. Hence we opted to by-pass the perception layer and directly obtain localization estimates and perceived objects from the simulation platform. This "ground truth" information can be further modified by introducing noise and other imperfections, in order to recreate patterns of perception-related failures observed in the real world.</span></li>
 	<li style="font-weight: 400;"><b>Planning</b><span style="font-weight: 400;">: Since our focus is on modeling actor behavior, we further decompose the Planning layer into three modules: Route Planning, Behavior Planning, and Motion Planning.</span>
<ul>
 	<li style="font-weight: 400;"><b>Route Planning</b><span style="font-weight: 400;">: It is assumed that when a scenario is initiated, the scenario execution engine spawns and assigns a destination to each actor. The route planning module processes this input, and produces a high-level route to the destination, which may include segments of driving along in a lane, turning left/right, changing lanes, etc. It is also possible for the scenario execution engine to specify an explicit sequence of behaviors for an actor, in which case the route planning module is not activated.</span></li>
 	<li style="font-weight: 400;"><b>Behavioral Planning</b><span style="font-weight: 400;">: The behavioral planning module operates at the level of driving behaviors, such as lane-following, lane-changing, intersection negotiation, etc. As shown in the diagram below, it consists of three sub-modules:</span>
<ul>
 	<li style="font-weight: 400;"><b>Behavior Selector</b><span style="font-weight: 400;">: The behavior selector acts as the high-level decision-maker. If a destination is specified by the scenario execution engine, and a valid route is available, the behavior selector chooses an appropriate behavior for the current segment of driving. Alternatively, if the execution engine explicitly specifies a behavior, that choice overrides the selection criteria, e.g. the scenario may call for a lane-change in the middle of an intersection even if it is not prudent to do so.</span></li>
 	<li style="font-weight: 400;"><b>Behaviors</b><span style="font-weight: 400;">: The core behaviors provide abstract driving actions such as lane-following, car-following, lane-change, signalized/unsignalized intersection negotiation, and stop/yield. These may be composed of lower-level maneuvers such as accelerate, cruise, decelerate, turn, etc., which may be shared across core behaviors. Some of the behaviors, such as lane-change and yield are anticipative in nature, which need to take into account the predicted trajectories of other actors. For simplicity, we use a physics-based short-term prediction engine for this purpose, </span><span style="font-weight: 400;">but for more accurate results, a dedicated interaction-aware prediction module can also be introduced into the system.</span></li>
 	<li style="font-weight: 400;"><b>Behavior Post-Processor</b><span style="font-weight: 400;">: The outputs of the different behaviors may be significantly different in nature. For instance, a car-following behavior may output a single target acceleration value, whereas a lane-change behavior may produce a target waypoint in the next lane. The post-processing step converts these different representations into a common form that can be passed onto the next stage (namely, Motion Planning). Also, if multiple behaviors are active at the same time (unusual, but possible), then the post-processor can combine/arbitrate between their outputs.</span></li>
</ul>
</li>
 	<li style="font-weight: 400;"><b>Motion Planning</b><span style="font-weight: 400;">: Given a desired motion specification from the behavior planning layer (e.g. a target way-point and velocity), this module is resp</span><span style="font-weight: 400;">onsible for computing a feasible, comfortable trajectory. This layer can additionally be responsible for reactive obstacle avoidance.</span></li>
</ul>
</li>
 	<li style="font-weight: 400;"><b>Controls</b><span style="font-weight: 400;">: The controls module is responsible for making the vehicle move along the trajectory produced by the motion planner, using typical throttle, brake and steering inputs. This can be a simple PID controller with adjustable parameters (as demonstrated in the basic agent </span><a href="https://github.com/carla-simulator/carla/blob/0.9.9/PythonAPI/carla/agents/navigation/controller.py"><span style="font-weight: 400;">controller</span></a><span style="font-weight: 400;"> included with </span><span style="font-weight: 400;">CARLA</span><span style="font-weight: 400;">), or a more sophisticated model-predictive controller (MPC).</span></li>
</ul>
<span style="font-weight: 400;"><b><img class="wp-image-815 aligncenter" style="font-weight: 400;" src="https://auro.ai/wp-content/uploads/2020/10/Behavior_Planning_Overview_v3.png" alt="" width="78%" /></b>Human-like failures or imperfections can be injected in different layers of the architecture. As an example, drivers do not always stop precisely at a stop sign, often dangerously crossing into the intersection; this deviation can be introduced into the behavior-based actor model by adding some noise and/or delay at different stages, including Perception (failure to perceive the stop sign), Planning (comfort parameters producing insufficient </span><span style="font-weight: 400;">deceleration) or Controls (imperfect/uncalibrated braking).</span>
<h2><span style="font-weight: 400;">Modeling Signalized Intersection Behaviors</span></h2>
<span style="font-weight: 400;">The main advantage of Behavior Trees (BTs) is that they are designed to be modular, hierarchical and easily extensible. Mathematically, they are structured as Directed Acyclic Graphs (DAGs). As a result, you can add a new "node" or move one around in a BT to change its behavior, without having to worry about other parts of the tree. This is in contrast with formalisms such as Finite State Machines (FSMs), which often require significant rework to the entire model when adding a new state or changing the co</span><span style="font-weight: 400;">nnectivity of an existing one.</span>

<span style="font-weight: 400;">The hierarchical structure of a BT allows control to flow down to leaf nodes and back up as the nodes finish their tasks. This allows for conditional behaviors to be specified more easily, e.g. if a lane-change behavior fails to execute because of a vehicle in the next lane, the actor can fall back to a lane-follow behavior. FSMs, on the other hand, provide a one way control transfer, in the sense that they have no memory of where a transition from one state to the other was originally made from [</span><a href="https://ieeexplore.ieee.org/abstract/document/6942752"><span style="font-weight: 400;">8</span></a><span style="font-weight: 400;">]. Implementing conditional behaviors would require additional states and transitions, making the state machine unnecessarily complex.</span>

<span style="font-weight: 400;">We start with a library of reusable nodes that provide a scalable, intuitive, and flexible way of defining composite behaviors. Adding a new core driving behavior consists of defining a sub-tree of these nodes, along with any new nodes specific to the behavior. The complete BT for a comprehensive actor model can become quite large, but it can still be analyzed and tested in terms of sub-trees and nodes. As an example, two snippets of signalized intersection behaviors are highlighted below (unprotected left turn at a green light and proceeding straight at a yellow light). Based on the behavior selected by our scenario selector at signalized intersections, the root of the appropriate BT is activated. If an actor is going through a traffic light, for example, this activation is progressed down the traffic light sub-tree, reaching a leaf node that can be either an Action (depicted as a simple rectangle) or a Condition (depicted as an oval). Each leaf node returns either a Running, Success or Failure status, which is passed back up the tree reaching the root (of the sub-tree).</span>
<div style="text-align: center;"><video src="https://auro.ai/wp-content/uploads/2020/10/Signalized_Intersection_Behavior_Tree_v8.mp4" autoplay="autoplay" loop="loop" controls="controls" width="95%" height="95%"></video></div>
<i><span style="font-weight: 400;">Note: In the BT diagrams shown above, a Sequence node (depicted by →) runs through its children in order on every cycle or "tick" until a child returns Failure or Running status [</span></i><a href="https://arxiv.org/pdf/1709.00084.pdf"><i><span style="font-weight: 400;">9</span></i></a><i><span style="font-weight: 400;">]. A Selector (depicted by ?), runs its children in order until a child returns Success or Running. For more information on the different constructs and idioms used in BTs, check out the excellent documentation for the </span></i><a href="https://py-trees.readthedocs.io/en/devel/"><i><span style="font-weight: 400;">py_trees</span></i></a><i><span style="font-weight: 400;"> package.</span></i>

&nbsp;

<b>Unprotected Left at a Green Light</b><span style="font-weight: 400;">: Turning situations complicate traffic modeling at signalized intersections. That’s why a separate BT is solely dedicated to govern the corresponding behaviors. The enlarged snippet above illustrates a portion of the BT that is active when a behavior-based actor is turning left at a green traffic light. As you can see, it simply verifies that the traffic light applicable to us is Green, and then re-uses the unsignalized intersection behavior. This was originally designed to handle intersections controlled by Stop or Yield signs; however, we can benefit from this model in the left-turn-at-green scenario by assuming that: i) the ego actor and all other actors don’t have a stop sign, and ii) other actors have the Right-of-Way (ROW) over the ego actor.</span>

<span style="font-weight: 400;">Some of the parameters that we used in this specific behavior - unprotected left at a green light - are as follows:</span>
<ul>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">traffic</span><span style="font-weight: 400;">_light_identif</span><span style="font-weight: 400;">y</span><span style="font-weight: 400;"><sub>range</sub></span></em><span style="font-weight: 400;">: How far is the traffic light’s status detectable (human-like vision range).</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">is</span><span style="font-weight: 400;">_allowerd_to_enter_intersection</span></em><span style="font-weight: 400;">: Boolean value to determine whether an actor is allowed to pull into the intersection while yielding to the thru traffic. Pulling into the intersection may also be restricted by other factors.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">time</span><span style="font-weight: 400;">_to_collision_safe_margi</span><span style="font-weight: 400;">n</span><span style="font-weight: 400;"><sub>travel_time</sub></span></em><span style="font-weight: 400;">: Safety margin for the time-to-collision model that is added to the actor’s travel time to the collision point. Larger values will create more conservative behavior.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">t</span><span style="font-weight: 400;">ime</span><span style="font-weight: 400;">_to_collision_safe_margi</span><span style="font-weight: 400;">n</span><span style="font-weight: 400;"><sub>distance</sub></span></em><span style="font-weight: 400;">: Safety margin for the time-to-collision model that inflates/deflates the imaginary collision avoidance area surrounding each conflicting  actor. Larger values will tend to leave more gap between actors.</span></li>
</ul>
<span style="font-weight: 400;">A range of diverse behaviors can be obtained by changing these parameters, as shown below.</span>
<p style="text-align: center;"><video src="https://auro.ai/wp-content/uploads/2020/10/green_left_turn_v1.mp4" autoplay="autoplay" loop="loop" controls="controls" width="95%" height="95%"><span data-mce-type="bookmark" style="width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start">﻿</span></video>
<em>Just by changing a few parameters, a variety of human-like simulation scenarios are achieved for an actor turning left at a green traffic light. Different levels of aggressiveness are shown here for the white vehicle (for demonstration purposes, all the demonstrated vehicles, including the ego white vehicle, are driven by our behavior-based actor controller).</em></p>
&nbsp;

<b>Straight at a Yellow Light</b><span style="font-weight: 400;">: When approaching a traffic light, if the light turns from green to yellow, the behavior of an actor is primarily determined by the "travel time" remaining to reach the intersection. If this time is small enough (usually less than the duration of the yellow light), then the actor can clearly make it through and so the Cross node is activated. If it is too long, then the actor clearly can't make it and so the Stop/Yield node is activated. This leaves a certain range of travel time which creates a "dilemma zone" where the choice of whether to go through or not is random, governed by a specified probability (which is also reflective of the "aggressiveness" of the actor in yellow-light situations).</span>

<span style="font-weight: 400;">Here we summarize the parameters that are used in this specific behavior </span><span style="font-weight: 400;">—</span><span style="font-weight: 400;"> i.e. when approaching a yellow traffic light:</span>
<ul>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">no</span><span style="font-weight: 400;">_</span><span style="font-weight: 400;">d</span><span style="font-weight: 400;">ilemma_travel_time</span><span style="font-weight: 400;"><sub>lb</sub> </span></em><span style="font-weight: 400;">: Actors with lower travel time to the stop-line will proceed with no dilemma.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">no</span><span style="font-weight: 400;">_</span><span style="font-weight: 400;">d</span><span style="font-weight: 400;">ilemma_travel_time</span><span style="font-weight: 400;"><sub>ub </sub></span></em><span style="font-weight: 400;">: Actors with greater travel time to the stop-line will stop with no dilemma.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">d</span><span style="font-weight: 400;">ilemma_stopping_probability</span><span style="font-weight: 400;"><sub>thr </sub></span></em><span style="font-weight: 400;">: Actors with larger stopping probability are more likely to stop.</span></li>
</ul>
<span style="font-weight: 400;">A variety of human-like behaviors at yellow light (including running a red light!) were obtained by varying these parameters, as shown below. In the next section, we will demonstrate how such parameters can be fit to real-world traffic data.</span>
<div style="max-width: 3838px; text-align: center;">
<div style="text-align: center;"><video src="https://auro.ai/wp-content/uploads/2020/10/yel_dilemma_grid.mp4" autoplay="autoplay" loop="loop" controls="controls" width="100%" height="100%"></video></div>
</div>
<p style="text-align: center;"><em><span style="font-weight: 400;">With our customized behavior-based actor (demonstrated here by the white vehicle), a variety of human-like simulation scenarios are achievable just at the yellow traffic light itself. The integrated human driving model creates a dilemma zone where the choice of whether to go through the intersection or not is unpredictable. On the other hand, the CARLA simulator’s simple traffic agent [</span><a href="https://github.com/carla-simulator/carla/blob/master/PythonAPI/carla/agents/navigation/basic_agent.py"><span style="font-weight: 400;">7</span></a><span style="font-weight: 400;">], demonstrated here by the black vehicle, produces predictable and easily replicable behaviors.</span></em></p>

<h2><span style="font-weight: 400;">Data-Driven Traffic Modeling - for simulating behavior in dilemma zones </span></h2>
<span style="font-weight: 400;"><img class="alignright size-full wp-image-854" src="https://auro.ai/wp-content/uploads/2020/10/yellow_dilemma_v6.png" alt="" width="45%" height="45%" />For testing, verification, and training our self-driving cars in simulation, it is critical to utilize real traffic data to drive our customized simulated actors and reproduce their surrounding environment. The simulation output needs to fairly match the original traffic conditions not only statistically but also on an individual scenario basis. This data-driven approach and its benefits for our application can be better explained by an example of the human-like behavior modeling at yellow traffic lights. We created scenarios/situations for our behavior-based actor in the </span><a href="http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf"><span style="font-weight: 400;">CARLA</span></a><span style="font-weight: 400;"> simulator similar to that of a motorist who will have difficulty making the correct stop/go decision when approaching a yellow light at signalized intersections.</span>

<span style="font-weight: 400;">In this context, we would like to rephrase the main features of our behavior-based actor as follows:</span>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The simulated actor provides various levels of uncertainty when making the stop or go decision (as depicted in the scenarios shown above).</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The characteristics of the simulated actor can be easily tuned/changed by altering behavior-specific parameters - this is what allowed us to produce a red light running scenario without explicitly forcing the actor to ignore the traffic light.</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The probability of the simulated actor’s choice of crossing through at yellow light is extracted from real-world data.</span></li>
</ul>
<img class="alignright wp-image-856" src="https://auro.ai/wp-content/uploads/2020/10/yellow_dilemma_prob_dist_v6.png" alt="" width="30%" height="30%" />
<span style="font-weight: 400;">We are interested in predicting the likelihood that a motorist decides to stop or cross the intersection at a yellow light. The concept of "travel time" introduced in the previous section can be further enhanced by modeling the decision as some function of the actor's current velocity, and distance to the associated stop-line demarcating the threshold for the intersection.</span>

<span style="font-weight: 400;">Because the dependent variable in this function is dichotomous (stop/go), a Binary Logistic Regression is useful and a linear regression is not an appropriate model (</span><i><span style="font-weight: 400;">y</span></i><span style="font-weight: 400;">=1 represents that the actor chooses to stop, while </span><i><span style="font-weight: 400;">y</span></i><span style="font-weight: 400;">=0 represents that the actor proceeds through the intersection) [</span><a href="https://www.hindawi.com/journals/mpe/2014/518782/"><span style="font-weight: 400;">4</span></a><span style="font-weight: 400;">, </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5201354/"><span style="font-weight: 400;">5</span></a><span style="font-weight: 400;">]. A new dependent variable \(Logit(P)\)</span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">is created as proposed in [</span><a href="https://www.hindawi.com/journals/mpe/2014/518782/"><span style="font-weight: 400;">4</span></a><span style="font-weight: 400;">]</span><i><span style="font-weight: 400;">, </span></i><span style="font-weight: 400;">where </span><i><span style="font-weight: 400;">P(y=1) </span></i><span style="font-weight: 400;"> is the probability of a motorist’s choice of stopping, similarly </span><i><span style="font-weight: 400;">P(y=0)</span></i><span style="font-weight: 400;"> is the probability of the motorist's choice of crossing through, and \(Logit(P)\) </span><span style="font-weight: 400;">is the natural log of odds of </span><i><span style="font-weight: 400;">y=1 vs y=0:</span></i>
<p style="padding-left: 50px;">\(Logit(P)=ln[\frac{P(y=1)}{1-P(y=1)}]\)</p>
<p style="text-align: left;"><span style="font-weight: 400;">Assuming that \(Logit(P)\) </span><span style="font-weight: 400;">is a linear function of the distance of the vehicle to the stop bar (</span><i><span style="font-weight: 400;">d</span></i><i><span style="font-weight: 400;"><sub>stopbar</sub></span></i><i><span style="font-weight: 400;">), </span></i><span style="font-weight: 400;">and the vehicle velocity (</span><i><span style="font-weight: 400;">v</span></i><span style="font-weight: 400;">), we have</span><i><span style="font-weight: 400;">:</span></i></p>
<p style="padding-left: 50px;">\(ln[\frac{P(y=1)}{1-P(y=1)}]=b_0+b_1\times v+b_2\times d_{stopbar}\)</p>
<p style="padding-left: 50px;"><i></i><i><span style="font-weight: 400;">\(P(y=1) = \frac{e^{b_0 + b_1\times v+ b_2\times d_{stopbar}}}{1+e^{b_0 + b_1\times v+ b_2\times d_{stopbar}}}\)</span></i></p>
<p style="padding-left: 50px;"><i><span style="font-weight: 400;">\(P(y=0) = 1- P(y=1)\)</span></i></p>
<span style="font-weight: 400;">Here, the function is characterized by 3 regression coefficients ((<i>b</i><i><sub>0</sub></i></span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">, <i>b</i><i><sub>1</sub></i></span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">, <i>b</i><i><sub>2</sub></i></span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">). This model was already fitted to a dataset of recorded videos collected by [</span><a href="https://www.researchgate.net/publication/257356834_Impact_of_countdown_timer_on_driving_maneuvers_after_the_yellow_onset_at_signalized_intersections_An_empirical_study_in_Changsha_China"><span style="font-weight: 400;">6</span></a><span style="font-weight: 400;">] in Changchun, China. The obtained stopping probabilities are plotted here for <i>b</i><i><sub>0</sub></i></span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">=-1.552 , <i>b</i><i><sub>1</sub></i></span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">=-0.050, and <i>b</i><i><sub>2</sub></i></span><i><span style="font-weight: 400;"> </span></i><span style="font-weight: 400;">=0.151 [<a href="https://www.hindawi.com/journals/mpe/2014/518782/">4</a>].</span>
<h2><span style="font-weight: 400;">What to expect next?</span></h2>
<span style="font-weight: 400;">Armed with a hierarchically structured actor model that represents a wide range of driving behaviors which is successfully integrated with a simulator, we are widening the spectrum of driving behaviors over time, increasing diversity within each behavior, and injecting uncertainty at various levels of processing (including perception). We are simulating erratic human behaviors in terms of imperfect actions that include drunk driving and loss of control.</span>

<span style="font-weight: 400;">The next blog post in this series will take you more deeper into the aspects of data-driven traffic modeling. To make simulations more realistic, we are extracting parameterized distributions for different driving behaviors from real-world data. Some of the more well-studied behaviors such as car-following and lane-changing can be modeled using existing traffic datasets; however, a comprehensive model will require data at scale, especially from urban driving environments. This is where we are leveraging the data collection and processing technology that we have developed for the various shared mobility fleets being operated by Ridecell's platform across the world.</span>
